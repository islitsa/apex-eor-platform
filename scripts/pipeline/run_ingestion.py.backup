"""
Data Ingestion Pipeline Runner

Main orchestration script for the complete data ingestion pipeline.

Runs three main phases:
1. DOWNLOAD - Fetch data from external sources
2. EXTRACT - Uncompress and extract archives
3. PARSE - Convert to structured formats (CSV/Parquet)

Usage:
    # Run all phases for all datasets
    python scripts/pipeline/run_ingestion.py --all

    # Run specific phases
    python scripts/pipeline/run_ingestion.py --download --extract --parse

    # Run for specific datasets
    python scripts/pipeline/run_ingestion.py --datasets rrc_production fracfocus

    # Force re-download
    python scripts/pipeline/run_ingestion.py --download --force

    # Dry run (show what would be done)
    python scripts/pipeline/run_ingestion.py --all --dry-run
"""

import argparse
import sys
from pathlib import Path
from dotenv import load_dotenv

# Load environment variables FIRST before any other imports
load_dotenv()
from datetime import datetime
from typing import List, Dict, Optional

# Add parent directory to path
SCRIPTS_DIR = Path(__file__).parent.parent
PROJECT_ROOT = SCRIPTS_DIR.parent
sys.path.insert(0, str(SCRIPTS_DIR))
sys.path.insert(0, str(PROJECT_ROOT))

from downloaders.rrc_downloader import RRCDownloader
from downloaders.fracfocus_downloader import FracFocusDownloader
from pipeline.extract import ExtractionOrchestrator
from pipeline.parse import ParsingOrchestrator


class IngestionPipeline:
    """Main data ingestion pipeline orchestrator"""

    AVAILABLE_DATASETS = [
        'rrc_production',
        'rrc_permits',
        'rrc_completions',
        'fracfocus'
    ]

    def __init__(self, base_data_dir: str = 'data/raw', dry_run: bool = False):
        """
        Initialize ingestion pipeline

        Args:
            base_data_dir: Base directory for raw data
            dry_run: If True, only show what would be done
        """
        self.base_data_dir = Path(base_data_dir)
        self.dry_run = dry_run

        # Initialize components
        self.rrc_downloader = RRCDownloader(str(self.base_data_dir / 'rrc'))
        self.fracfocus_downloader = FracFocusDownloader(str(self.base_data_dir / 'fracfocus'))
        self.extractor = ExtractionOrchestrator(str(self.base_data_dir))
        self.parser = ParsingOrchestrator(str(self.base_data_dir))

        self.results = {
            'download': {},
            'extract': {},
            'parse': {}
        }

        self.ui_agent = None  # Will be initialized if UI is requested

    def run_download(self, datasets: Optional[List[str]] = None, force: bool = False) -> Dict[str, bool]:
        """
        Run download phase

        Args:
            datasets: List of datasets to download (None = all)
            force: Force re-download even if files exist

        Returns:
            Dictionary with download results
        """
        print("\n" + "="*70)
        print("PHASE 1: DOWNLOAD")
        print("="*70)

        if self.dry_run:
            print("[DRY RUN] Would download the following datasets:")
            for dataset in (datasets or self.AVAILABLE_DATASETS):
                print(f"  - {dataset}")
            return {}

        results = {}

        # Determine which datasets to download
        download_all = datasets is None or len(datasets) == 0

        # RRC Production
        if download_all or 'rrc_production' in datasets:
            print("\n--- Downloading RRC Production ---")
            results['rrc_production'] = self.rrc_downloader.download_production(force)

        # RRC Permits
        if download_all or 'rrc_permits' in datasets:
            print("\n--- Downloading RRC Permits ---")
            results['rrc_permits'] = self.rrc_downloader.download_permits(force)

        # RRC Completions
        if download_all or 'rrc_completions' in datasets:
            print("\n--- Downloading RRC Completions ---")
            results['rrc_completions'] = self.rrc_downloader.download_completions(force)

        # FracFocus
        if download_all or 'fracfocus' in datasets:
            print("\n--- Downloading FracFocus ---")
            results['fracfocus'] = self.fracfocus_downloader.download_csv_bulk(force)

        self.results['download'] = results
        return results

    def run_extract(self, datasets: Optional[List[str]] = None) -> Dict[str, bool]:
        """
        Run extraction phase

        Args:
            datasets: List of datasets to extract (None = all)

        Returns:
            Dictionary with extraction results
        """
        print("\n" + "="*70)
        print("PHASE 2: EXTRACT")
        print("="*70)

        if self.dry_run:
            print("[DRY RUN] Would extract the following datasets:")
            for dataset in (datasets or self.AVAILABLE_DATASETS):
                print(f"  - {dataset}")
            return {}

        results = {}

        # Determine which datasets to extract
        extract_all = datasets is None or len(datasets) == 0

        # RRC Production
        if extract_all or 'rrc_production' in datasets:
            print("\n--- Extracting RRC Production ---")
            results['rrc_production'] = self.extractor.extract_rrc_production()

        # RRC Permits
        if extract_all or 'rrc_permits' in datasets:
            print("\n--- Extracting RRC Permits ---")
            results['rrc_permits'] = self.extractor.extract_rrc_permits()

        # RRC Completions
        if extract_all or 'rrc_completions' in datasets:
            print("\n--- Extracting RRC Completions ---")
            results['rrc_completions'] = self.extractor.extract_rrc_completions()

        # FracFocus
        if extract_all or 'fracfocus' in datasets:
            print("\n--- Extracting FracFocus ---")
            results['fracfocus'] = self.extractor.extract_fracfocus()

        self.results['extract'] = results
        return results

    def run_parse(self, datasets: Optional[List[str]] = None) -> Dict[str, bool]:
        """
        Run parsing phase

        Args:
            datasets: List of datasets to parse (None = all)

        Returns:
            Dictionary with parsing results
        """
        print("\n" + "="*70)
        print("PHASE 3: PARSE")
        print("="*70)

        if self.dry_run:
            print("[DRY RUN] Would parse the following datasets:")
            for dataset in (datasets or self.AVAILABLE_DATASETS):
                print(f"  - {dataset}")
            return {}

        results = {}

        # Determine which datasets to parse
        parse_all = datasets is None or len(datasets) == 0

        # RRC Production
        if parse_all or 'rrc_production' in datasets:
            print("\n--- Parsing RRC Production ---")
            results['rrc_production'] = self.parser.parse_rrc_production()

        # RRC Permits
        if parse_all or 'rrc_permits' in datasets:
            print("\n--- Parsing RRC Permits ---")
            results['rrc_permits'] = self.parser.parse_rrc_permits()

        # RRC Completions
        if parse_all or 'rrc_completions' in datasets:
            print("\n--- Parsing RRC Completions ---")
            results['rrc_completions'] = self.parser.parse_rrc_completions()

        # FracFocus
        if parse_all or 'fracfocus' in datasets:
            print("\n--- Parsing FracFocus ---")
            results['fracfocus'] = self.parser.parse_fracfocus()

        self.results['parse'] = results
        return results

    def run_all(self, datasets: Optional[List[str]] = None, force: bool = False):
        """
        Run all pipeline phases (download, extract, parse)

        Args:
            datasets: List of datasets to process (None = all)
            force: Force re-download
        """
        start_time = datetime.now()

        print("\n" + "="*70)
        print("DATA INGESTION PIPELINE")
        print("="*70)
        print(f"Start time: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"Datasets: {', '.join(datasets) if datasets else 'ALL'}")
        print(f"Dry run: {self.dry_run}")
        print(f"Force download: {force}")
        print("="*70)

        # Run phases
        self.run_download(datasets, force)
        self.run_extract(datasets)
        self.run_parse(datasets)

        # Print final summary
        end_time = datetime.now()
        duration = end_time - start_time

        print("\n" + "="*70)
        print("PIPELINE COMPLETE")
        print("="*70)
        print(f"End time: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"Duration: {duration}")
        print("\nResults by phase:")

        for phase, results in self.results.items():
            if results:
                print(f"\n{phase.upper()}:")
                for dataset, success in results.items():
                    status = "âœ“" if success else "âœ—"
                    print(f"  {status} {dataset}")

        print("="*70)

    def launch_ui(self, use_assembly: bool = False):
        """
        Launch Gradio dashboard using Opus MVP architecture

        Args:
            use_assembly: Ignored (Opus MVP always uses assembly)
        """
        print("\n" + "="*70)
        print("AGENT-POWERED DASHBOARD GENERATOR")
        print("="*70)

        try:
            import subprocess
            import sys
            from pathlib import Path

            print("\nLaunching Agent Studio (3-Column Interface):")
            print("  [+] LEFT: Your profile, save chats, history")
            print("  [+] MIDDLE: Chat with agents, ask questions")
            print("  [+] RIGHT: Watch agents collaborate in real-time")
            print("="*70)
            print("\n[LAUNCH] Opening Agent Studio...")
            print("   - Load context with the button in left column")
            print("   â€¢ Click 'Generate Dashboard' in middle column to start")
            print("   â€¢ Watch agents talk to each other in right column")
            print("\n")

            # Launch Streamlit Agent Studio (3-column interface)
            project_root = Path(__file__).parent.parent.parent
            streamlit_file = project_root / "src" / "ui" / "agent_studio.py"

            # Launch Agent Studio - 3 columns: profile + your chat + agent chat
            # Pass --force-llm to skip snippets and use full agent generation
            # This lets you chat with agents about design decisions
            subprocess.run([
                sys.executable, "-m", "streamlit", "run",
                str(streamlit_file),
                "--server.headless", "false",
                "--",
                "--force-llm"  # Skip snippets, use full agent design
            ])

        except KeyboardInterrupt:
            print("\n\nDashboard stopped by user")
        except Exception as e:
            print(f"\nERROR generating dashboard: {e}")
            import traceback
            traceback.print_exc()
            sys.exit(1)

            # Build the requirements prompt
            requirements = f"""
Create a production-ready Gradio dashboard for the APEX Data Ingestion Pipeline.

The pipeline has three phases: DOWNLOAD -> EXTRACT -> PARSE

DATASETS:
{', '.join(self.AVAILABLE_DATASETS)}

REQUIRED FEATURES:
1. Dataset Selection
   - Checkboxes for each dataset
   - Default to selecting the first dataset

2. Pipeline Controls
   - Button: "Start Download" - calls pipeline.run_download(datasets, force=force_redownload)
   - Button: "Start Extract" - calls pipeline.run_extract(datasets)
   - Button: "Start Parse" - calls pipeline.run_parse(datasets)
   - Button: "Run All Phases" (primary, large) - calls pipeline.run_all(datasets, force=force_redownload)
   - Checkbox: "Force Re-download"
   - Checkbox: "Dry Run"

3. Status Display
   - Show current pipeline status
   - Show which phase is active
   - Use colored status indicators (âœ… âŒ ðŸ”„ âšª)

4. Logs Viewer
   - Scrollable textbox showing pipeline logs
   - Update logs after each operation
   - Show timestamps

5. Metrics Display
   - Total records processed
   - File size (MB)
   - Duration (minutes)
   - Last run timestamp

LAYOUT:
- Left column: Dataset selection, options, metrics
- Right column: Status, control buttons, phase progress, logs

CRITICAL REQUIREMENTS:
1. The pipeline object is already available in the namespace as `pipeline`.
   DO NOT create a MockPipeline or any test pipeline class.
   DO NOT write: pipeline = MockPipeline() or pipeline = anything
   The real pipeline instance is injected - just use `pipeline` directly.
   Use try/except blocks around pipeline calls and show errors in the logs.

2. DO NOT import any custom utility modules like utils.metrics_reader, utils.status_tracker, etc.
   ALL CODE MUST BE SELF-CONTAINED IN A SINGLE FILE.
   Only use standard library imports (json, os, pathlib, datetime) and gradio.
   Write all helper functions directly in the same file.

3. DO NOT use emojis anywhere in the code (âœ… âŒ ðŸ”„ etc).
   Use simple text indicators instead: [COMPLETE], [RUNNING], [ERROR], [PENDING]
   This is critical for Windows encoding compatibility.
   Keep the design clean and professional with text-only status indicators.

READING REAL METRICS:
The real pipeline has metadata.json files you can read to show actual metrics:
- Read data/raw/rrc/production/metadata.json for RRC production data
- Read data/raw/rrc/completions_data/metadata.json for RRC completions data
- Read data/raw/rrc/horizontal_drilling_permits/metadata.json for RRC permits data
- Read data/raw/fracfocus/metadata.json for FracFocus data
- Parse the "parsed" section (NOT "parsing_results") to get total_rows, total_size_bytes, etc.
- Show these REAL metrics in the dashboard from the metadata files
- Example: metadata['parsed']['total_rows'], metadata['parsed']['total_size_bytes']
"""

            # Build pipeline context
            pipeline_context = {
                'datasets': self.AVAILABLE_DATASETS,
                'phases': ['download', 'extract', 'parse'],
                'status': 'idle',
                'methods': [
                    'run_download(datasets, force)',
                    'run_extract(datasets)',
                    'run_parse(datasets)',
                    'run_all(datasets, force)'
                ]
            }

            # Generate the UI code with evolution
            print("\n" + "="*70)
            print("STARTING EVOLUTIONARY GENERATION")
            print("="*70)
            print("Max Iterations: 3")
            print("Target Quality Score: 8.0 / 10")
            print("="*70 + "\n")

            result = self.ui_agent.generate_ui_with_evolution(
                description=requirements,
                framework="gradio",
                include_backend=False,
                max_iterations=3,
                target_score=8.0,
                save_log=True
            )

            # Display results
            print("\n" + "="*70)
            print("EVOLUTION COMPLETE")
            print("="*70)
            print(f"Initial Score:  {result['evolution_history'][0]['score']:.1f} / 10" if result.get('evolution_history') else "Initial Score:  N/A")
            print(f"Final Score:    {result['final_score']:.1f} / 10")
            print(f"Improvement:    +{result['improvement']:.1f} points")
            print(f"Iterations:     {result['total_iterations']}")
            print(f"Target Reached: {'[YES]' if result['target_reached'] else '[NO]'}")
            print("="*70 + "\n")

            # Save ORIGINAL (first iteration) for comparison
            if result.get('evolution_history') and len(result['evolution_history']) > 0:
                original_code = result['evolution_history'][0].get('code', '')
                if original_code:
                    original_path = PROJECT_ROOT / "generated_pipeline_dashboard_v1_original.py"
                    with open(original_path, 'w', encoding='utf-8') as f:
                        f.write(original_code)
                    print(f"[Saved] Original dashboard (v1): {original_path}")
                    print(f"        Score: {result['evolution_history'][0]['score']:.1f}/10")
                    print(f"        Size: {len(original_code):,} characters\n")

            # Save FINAL (evolved) dashboard
            dashboard_path = PROJECT_ROOT / f"generated_pipeline_dashboard_v{result['total_iterations']}_final.py"
            with open(dashboard_path, 'w', encoding='utf-8') as f:
                f.write(result['final_code'])

            print(f"[Saved] Final dashboard (v{result['total_iterations']}): {dashboard_path}")
            print(f"        Score: {result['final_score']:.1f}/10")
            print(f"        Size: {len(result['final_code']):,} characters")
            print(f"        Improvement: +{result['improvement']:.1f} points\n")

            # Test the dashboard before launching
            print("="*70)
            print("TESTING DASHBOARD FUNCTIONALITY")
            print("="*70)

            try:
                # Test 1: Verify Python syntax
                print("[ 1/4 ] Testing Python syntax... ", end='')
                compile(result['final_code'], str(dashboard_path), 'exec')
                print("[PASS]")

                # Test 2: Verify Gradio imports
                print("[ 2/4 ] Testing Gradio imports... ", end='')
                assert 'import gradio' in result['final_code'], "Missing Gradio import"
                print("[PASS]")

                # Test 3: Verify self-contained (no custom imports)
                print("[ 3/4 ] Testing self-contained code... ", end='')
                assert 'from utils.' not in result['final_code'], "Contains utils imports"
                assert 'from helpers.' not in result['final_code'], "Contains helpers imports"
                print("[PASS]")

                # Test 4: Verify button handlers exist
                print("[ 4/4 ] Testing button handlers... ", end='')
                assert 'def run_download' in result['final_code'] or 'run_download' in result['final_code'], "Missing run_download"
                assert 'def run_extract' in result['final_code'] or 'run_extract' in result['final_code'], "Missing run_extract"
                assert 'def run_parse' in result['final_code'] or 'run_parse' in result['final_code'], "Missing run_parse"
                print("[PASS]")

                print("\n[SUCCESS] ALL TESTS PASSED - Dashboard ready to launch!\n")

            except AssertionError as e:
                print(f"[FAIL]: {e}")
                print("\n[WARNING] Dashboard may have issues, but launching anyway...\n")
            except Exception as e:
                print(f"[ERROR]: {e}")
                print("\n[WARNING] Testing failed, but launching anyway...\n")

            # Launch the generated dashboard
            print("="*70)
            print("LAUNCHING DASHBOARD")
            print("="*70)
            print(f"URL: http://localhost:7863")
            print(f"Quality Score: {result['final_score']:.1f}/10")
            print("="*70 + "\n")

            import subprocess
            subprocess.Popen(["python", str(dashboard_path)])

        except ImportError as e:
            print(f"Error: Could not import Gradio: {e}")
            print("Make sure you have installed: pip install gradio")
        except Exception as e:
            print(f"Error launching UI: {e}")
            import traceback
            traceback.print_exc()

def main():
    """Main entry point"""
    parser = argparse.ArgumentParser(
        description='Data Ingestion Pipeline for APEX EOR Platform',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Run complete pipeline for all datasets
  python run_ingestion.py --all

  # Run only download and extract phases
  python run_ingestion.py --download --extract

  # Run for specific datasets
  python run_ingestion.py --all --datasets rrc_production fracfocus

  # Force re-download
  python run_ingestion.py --download --force

  # Dry run to see what would be done
  python run_ingestion.py --all --dry-run
        """
    )

    # Phase selection
    parser.add_argument('--all', action='store_true',
                        help='Run all phases (download, extract, parse)')
    parser.add_argument('--download', action='store_true',
                        help='Run download phase')
    parser.add_argument('--extract', action='store_true',
                        help='Run extract phase')
    parser.add_argument('--parse', action='store_true',
                        help='Run parse phase')

    # Dataset selection
    parser.add_argument('--datasets', nargs='+',
                        choices=IngestionPipeline.AVAILABLE_DATASETS,
                        help='Specific datasets to process (default: all)')

    # Options
    parser.add_argument('--force', action='store_true',
                        help='Force re-download even if files exist')
    parser.add_argument('--dry-run', action='store_true',
                        help='Show what would be done without executing')
    parser.add_argument('--base-dir', default='data/raw',
                        help='Base directory for raw data (default: data/raw)')

    # UI option
    parser.add_argument('--ui', action='store_true',
                        help='Launch web UI dashboard (uses UI agent to generate and open in browser)')
    parser.add_argument('--assembly', action='store_true',
                        help='Use assembly system for UI generation (20-70%% token savings)')

    args = parser.parse_args()

    # Initialize pipeline
    pipeline = IngestionPipeline(
        base_data_dir=args.base_dir,
        dry_run=args.dry_run
    )

    # If UI flag is set, launch UI and exit
    if args.ui:
        pipeline.launch_ui(use_assembly=args.assembly)
        return

    # Validate arguments for CLI mode
    if not (args.all or args.download or args.extract or args.parse):
        parser.error('Must specify at least one phase: --all, --download, --extract, --parse, or --ui')

    # Run requested phases
    if args.all:
        pipeline.run_all(args.datasets, args.force)
    else:
        if args.download:
            pipeline.run_download(args.datasets, args.force)
        if args.extract:
            pipeline.run_extract(args.datasets)
        if args.parse:
            pipeline.run_parse(args.datasets)


if __name__ == '__main__':
    main()
